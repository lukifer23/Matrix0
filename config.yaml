# Matrix0 V2 Production Configuration
# Optimized for performance and strong play

model:
  # V2 Architecture Features
  planes: 19                       # Input planes for chess board
  channels: 320                    # Number of channels in residual blocks (32M model)
  blocks: 24                       # Number of residual blocks (32M model)
  attention: true                  # Enable attention mechanisms
  attention_heads: 20              # Number of attention heads (32M model)
  policy_size: 4672                # Policy output size
  norm: "group"                    # GroupNorm instead of BatchNorm
  activation: "silu"               # SiLU activation instead of ReLU
  preact: true                     # Pre-activation residual blocks
  droppath: 0.0                    # Temporarily disable DropPath to avoid MPS tiny alloc OOMs
  policy_factor_rank: 128          # Factorized policy head (32M model)
  aux_policy_from_square: true     # Auxiliary from-square prediction
  aux_policy_move_type: true       # Auxiliary move-type prediction
  ssl_curriculum: true             # Self-supervised learning curriculum
  self_supervised: true            # Enable SSL
  ssl_tasks: ["piece", "threat", "pin", "fork", "control", "pawn_structure", "king_safety"]
  ssrl_tasks: []                   # Keep SSRL off for now (optional)

selfplay:
  num_workers: 3                   # Next pass: 3 workers
  batch_size: 256                  # Increased from 128 for better efficiency
  max_games: 420                    # TEST RUN: 20 games total
  max_game_len: 180                # Encourage decisive results
  min_resign_plies: 50             # Avoid premature resignations
  resign_threshold: -0.80          # Slightly earlier resigns to end lost games
  opening_random_plies: 8          # Randomize first N plies to reduce draws
  num_simulations: 200             # Increased from 160 for better quality
  draw:
    min_plies: 60                  # Require more play before draw checks
    window: 16                     # Wider window
    min_unique: 8                  # More unique positions
    halfmove_cap: 100              # FIDE fifty-move close but more tolerant

training:
  batch_size: 96                   # Stable baseline
  epochs: 2                        # Two epochs per cycle for deeper passes
  learning_rate: 0.001
  weight_decay: 0.0001
  checkpoint_dir: "checkpoints"    # Directory for checkpoints
  checkpoint: "checkpoints/best.pt"   # Always initialize from current best per cycle
  gradient_accumulation_steps: 2   # Effective batch 192
  progress_interval: 100           # Reduce per-step logging overhead
  grad_clip_norm: 0.5              # Stable
  ssl_weight: 0.15                 # Increased for better SSL learning contribution
  ssl_warmup_steps: 1000           # Faster warmup for SSL integration
  ssl_every_n: 1                   # Compute SSL every step for consistent learning
  ssl_chunk_size: 128              # Larger chunks for better efficiency
  ssl_target_weight: 1.0           # Normalize SSL magnitude
  ssl_tasks: ["piece", "threat", "pin", "fork", "control", "pawn_structure", "king_safety"]  # All SSL tasks enabled
  ssl_piece_weight: 1.0            # Weight for piece recognition
  ssl_threat_weight: 0.8           # Weight for threat detection
  ssl_pin_weight: 0.7              # Weight for pin detection
  ssl_fork_weight: 0.6             # Weight for fork detection
  ssl_control_weight: 0.5          # Weight for square control
  ssl_pawn_structure_weight: 0.4   # Weight for pawn structure
  ssl_king_safety_weight: 0.4      # Weight for king safety
  # removed duplicate ssl_chunk_size key
  precision: "fp16"                # Optimized mixed precision
  use_amp: true                    # AMP
  warmup_steps: 500                # Longer warmup for extended steps
  memory_limit_gb: 14              # MPS limit
  gradient_checkpointing_strategy: "adaptive"    # Enable only if memory is high
  steps_per_epoch: 12000           # Train over more samples per cycle
  dataloader_workers: 2            # Improve IO throughput
  prefetch_factor: 2               # Prefetch for DataLoader workers
  use_curriculum: true
  curriculum_phases:
    - { name: openings, steps: 1200, description: "Bootstrap with openings-heavy data" }
    - { name: tactics,  steps: 3600, description: "Emphasize tactical calculation" }
    - { name: mixed,    steps: 7600, description: "Blend external + self-play" }
  policy_masking: true             # Smart masking
  policy_label_smoothing: 0.05     # Mild label smoothing for policy CE
  value_loss: "huber"              # Robust value regression
  huber_delta: 1.0                 # Huber threshold
  extra_replay_dirs:
    - data/stockfish_games

orchestrator:
  initial_games: 210               # 3 workers × 70 games (more self-play data per cycle)
  subsequent_games: 420            # 3 workers × 140 games for stronger data growth
  games_per_cycle: 420             # Total games per cycle
  train_epochs_per_cycle: 1        # One epoch per cycle
  eval_games_per_cycle: 20         # More games per cycle for robust eval
  keep_top_k: 1                    # Keep only best model
  continuous_mode: true            # Continuous cycles enabled
  # Use latest best checkpoint automatically (no override)
  # checkpoint_override: best.pt
  tui: "table"                     # Use table display mode
  ui:
    compact: true                  # Child processes log to files; console stays clean

eval:
  games: 4                         # Light orchestrator eval; use enhanced_eval for deep
  num_simulations: 200             # Reduced from 800 for faster evaluation cycles
  max_moves: 160                   # Avoid early auto-draws
  external_engines: ["stockfish"]  # Compare against Stockfish
  tournament_rounds: 4             # More tournament rounds
  strength_estimation_games: 4     # More strength estimation games

engines:
  stockfish:
    path: /usr/local/bin/stockfish
    parameters:
      Threads: 2
      Hash: 256
      MultiPV: 1
    time_control: 100ms
    enabled: true
  matrix0:
    type: internal
    checkpoint: checkpoints/best.pt
    enabled: true

mcts:
  num_simulations: 200              # Increased from 96 for better accuracy
  cpuct: 2.2
  cpuct_start: 2.8
  cpuct_end: 1.8
  cpuct_plies: 40
  cpuct_c_base: 19652
  cpuct_c_init: 1.25
  dirichlet_alpha: 0.3
  dirichlet_frac: 0.25
  dirichlet_plies: 16
  selection_jitter: 0.05
  fpu: 0.2
  fpu_reduction: 0.15
  draw_penalty: -0.5
  tt_capacity: 1000000
  tt_cleanup_frequency: 500
  tt_memory_limit_mb: 1024
  batch_size: 16                    # Increased from 8 for better MPS utilization
  encoder_cache: true
  legal_softmax: true               # Softmax over legal moves only for priors
  max_children: 0
  min_child_prior: 0.0
  no_instant_backtrack: true
  parent_q_init: true
  tt_cleanup_interval_s: 5
  value_from_white: true            # Use side-to-move value consistently
  virtual_loss: 2.0
  enable_memory_cleanup: true
  memory_cleanup_threshold_mb: 512
  max_tree_nodes: 70000
  # FIXED: Parallel MCTS with robust error handling and deadlock prevention
  num_threads: 4                   # Increased from 2 for better CPU utilization
  parallel_simulations: true
  simulation_batch_size: 16        # Increased from 8 for better throughput
  tree_parallelism: true
  playout_random_frac: 0.3
  enable_entropy_noise: true

presets:
  mps:
    device: "mps"
    num_threads: 4  # Match worker count
    inference_batch_size: 32
    mcts_threads: 4  # Match worker count
    worker_threads: 1  # 1 core per worker for 4 workers
