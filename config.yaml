# Matrix0 V2 Production Configuration
# Optimized for performance and strong play

model:
  # V2 Architecture Features (≈45M params)
  planes: 19
  channels: 288                    # Reduced from 320 for faster inference
  blocks: 22                       # Reduced from 24
  attention: true
  attention_heads: 18              # Reduced from 20
  policy_size: 4672
  norm: "group"
  activation: "silu"
  value_activation: "leaky_relu"
  preact: true
  droppath: 0.0
  policy_factor_rank: 160          # Trim dense policy head cost
  infer_attention_stride: 2        # Inference-only: run every other attention block
  infer_amp_tower: true            # Inference-only: AMP for tower on MPS
  aux_policy_from_square: true
  aux_policy_move_type: true
  ssl_curriculum: true
  self_supervised: true
  ssl_tasks: ["piece", "threat", "pin", "fork", "control"]

selfplay:
  num_workers: 3                   # Default worker count
  batch_size: 256                  # Increased from 128 for better efficiency
  shared_inference_batch_size: 64  # Increased from 48 for better MPS batching
  max_games: 420                   # Self-play games per orchestration cycle
  max_game_len: 200                # Increased for more decisive games
  min_resign_plies: 50             # Increased for more mature positions
  resign_threshold: -0.85          # Less aggressive resignation
  opening_random_plies: 12         # More opening randomization
  num_simulations: 300             # Increased for better quality
  temperature_start: 1.2           # Increased from 1.0 for more early exploration
  temperature_end: 0.3             # Increased from 0.1 to maintain diversity longer
  temperature_moves: 40            # Increased from 20 to maintain diversity for 20 full moves
  draw:
    min_plies: 30                  # Reduced from 40 for earlier draw adjudication
    window: 8                      # Narrower window for faster detection
    min_unique: 4                  # Reduced for better draw detection
    halfmove_cap: 100              # FIDE fifty-move close but more tolerant

training:
  batch_size: 96                   # Stable baseline
  epochs: 2                        # Two epochs per cycle for deeper passes
  learning_rate: 0.001
  weight_decay: 0.0001
  checkpoint_dir: "checkpoints"    # Directory for checkpoints
  checkpoint: "checkpoints/best.pt"   # Always initialize from current best per cycle
  accum_steps: 2                   # Effective batch 192
  progress_interval: 100           # Reduce per-step logging overhead
  grad_clip_norm: 0.5              # Stable
  ssl_weight: 0.08                 # Reduced from 0.15 to balance with policy/value learning
  ssl_warmup_steps: 1000           # Faster warmup for SSL integration
  ssl_every_n: 1                   # Compute SSL every step for consistent learning
  ssl_chunk_size: 64               # Reduced for MPS memory stability
  ssl_target_weight: 1.0           # Normalize SSL magnitude
  # CRITICAL FIX: Force external data usage
  data_mode: "mixed"               # Use mixed mode to access external data
  dataloader_workers: 6            # Increase worker threads for faster data loading
  prefetch_factor: 2               # Maintain prefetching
  ssl_piece_weight: 1.0            # Weight for piece recognition
  ssl_threat_weight: 0.8           # Weight for threat detection
  ssl_pin_weight: 0.7              # Weight for pin detection
  ssl_fork_weight: 0.6             # Weight for fork detection
  ssl_control_weight: 0.5          # Weight for square control
  precision: "fp16"                # Optimized mixed precision
  use_amp: true                    # AMP
  warmup_steps: 500                # Longer warmup for extended steps
  memory_limit_gb: 14              # MPS limit
  gradient_checkpointing_strategy: "adaptive"    # Enable only if memory is high
  steps_per_epoch: 6000            # Shorter sanity runs per cycle
  use_curriculum: true
  curriculum_phases:
    - { name: openings, steps: 800, description: "Bootstrap with openings-heavy data" }
    - { name: tactics,  steps: 2400, description: "Emphasize tactical calculation" }
    - { name: mixed,    steps: 4800, description: "Blend external + self-play" }
  policy_masking: true             # Smart masking
  policy_label_smoothing: 0.1      # Increased from 0.05 for more policy entropy
  value_loss: "huber"              # Robust value regression
  huber_delta: 0.5                 # Reduced Huber threshold for stability
  extra_replay_dirs:
    - data/stockfish_games        # Curated Stockfish datasets (tagged)
    - data/tactical               # Processed tactical NPZ bundle
    - data/openings               # Processed openings NPZ bundle
    - data/teacher_games          # Teacher-guided replay data

orchestrator:
  initial_games: 210               # 3 workers × 70 games (more self-play data per cycle)
  subsequent_games: 420            # 3 workers × 140 games for stronger data growth
  games_per_cycle: 420             # Total games per cycle
  train_epochs_per_cycle: 1        # One epoch per cycle
  eval_games_per_cycle: 200        # Increased from 150 for tighter confidence interval
  keep_top_k: 1                    # Keep only best model
  continuous_mode: true            # Continuous cycles enabled
  promotion_threshold: 0.52        # Increased from 0.48 to require clear improvement
  # Use latest best checkpoint automatically (no override)
  # checkpoint_override: best.pt
  tui: "table"                     # Use table display mode
  ui:
    compact: true                  # Child processes log to files; console stays clean

# Endgame tablebases (Syzygy) for self-play adjudication and correctness
tablebases:
  enabled: true
  path: "data/syzygy"            # Place 3-5 man Syzygy tables here (wdl/)
  max_pieces: 5                   # Match currently installed tables to avoid missing-probe warnings

eval:
  games: 200                       # Increased from 150 for tighter confidence interval
  max_moves: 250                   # Allow more decisive games
  external_engines: ["stockfish"]  # Compare against Stockfish
  tournament_rounds: 6             # More tournament rounds
  strength_estimation_games: 6     # More strength estimation games
  openings_pgn: "data/openings/main_eval_book.pgn"  # Optional PGN book for varied starts

engines:
  stockfish:
    parameters:
      Threads: 2
      Hash: 256
      MultiPV: 1
    time_control: 100ms
  matrix0:
    type: internal

mcts:
  num_threads: 4                    # Re-enable parallel leaf collection for faster rollouts
  num_simulations: 300              # Increased for better evaluation quality
  cpuct: 2.5                        # Increased from 1.5 for much stronger exploration
  cpuct_start: 3.0                  # Increased from 1.8 for early-game exploration
  cpuct_end: 2.0                    # Increased from 1.2 to maintain exploration in endgame
  cpuct_plies: 40                   # Interpolate over first 40 plies
  dirichlet_alpha: 0.3
  dirichlet_frac: 0.25              # Increased from 0.15 (standard AlphaZero)
  dirichlet_plies: 30               # Increased from 10 for exploration throughout opening
  selection_jitter: 0.05
  fpu: 0.6                          # Increased from 0.4 for better unexplored node handling
  fpu_reduction: 0.1                # Reduced FPU reduction for more exploration
  draw_penalty: -0.05               # Increased from -0.02 for mild draw penalty
  tt_capacity: 1500000              # Tuned for MPS: reduce memory pressure
  tt_cleanup_frequency: 1000        # Tuned for MPS to reduce long cleanups
  tt_memory_limit_mb: 1024
  batch_size: 32                    # Tuned for MPS: smaller batches reduce latency
  mps_aggressive_stability: false   # New: gate MPS workarounds (disable overhead)
  encoder_cache: true
  legal_softmax: true               # Softmax over legal moves only for priors
  max_children: 0                   # Restore unlimited branching for full exploration
  min_child_prior: 0.0              # Restore original prior threshold
  no_instant_backtrack: true
  parent_q_init: true
  tt_cleanup_interval_s: 5
  value_from_white: false           # Model outputs side-to-move values (standard AlphaZero)
  virtual_loss: 2.0                 # Restore original virtual loss level
  enable_memory_cleanup: true
  # Throughput optimizations
  simulation_batch_size: 48         # Slightly larger mini-batches for MPS
  parallel_simulations: true        # Allow concurrent simulations on MPS
  tree_parallelism: true            # Allow tree-level parallelism
  memory_cleanup_threshold_mb: 512
  max_tree_nodes: 100000            # Increased for larger search trees
  playout_random_frac: 0.05         # Mild playout randomization for diversity
  enable_entropy_noise: true

presets:
  mps:
    device: "mps"
    worker_threads: 1  # 1 core per worker (3 workers)
