# GRPO Experiment Configurations
# Different setups for testing GRPO with various architectures

# Base configuration
base_config:
  device: "mps"
  random_seed: 42
  experiment_name: "grpo_base"

# Magnus Transformer + GRPO (70M Parameter Model - MAIN EXPERIMENT)
magnus_transformer_grpo:
  device: "mps"        # Use MPS for Apple Silicon GPU acceleration
  model:
    type: "magnus_transformer"  # Fixed model type mapping
    input_channels: 19
    d_model: 512       # Optimized embedding dimension
    nhead: 8           # 8 attention heads for balance
    num_layers: 12     # 12 transformer layers (deep but efficient)
    dim_feedforward: 2048  # Efficient feedforward dimension
    baseline_checkpoint: "checkpoints/baseline_magnus_transformer_fresh.pt"

  grpo:
    group_size: 8      # Larger groups for better normalization
    clip_epsilon: 0.2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    learning_rate: 3e-5  # Lower LR for stability
    max_grad_norm: 0.5
    ppo_epochs: 6      # More PPO epochs
    batch_size: 32     # Smaller batch for memory
    num_workers: 2     # Fewer workers due to model size
    mcts_simulations: 200  # Fewer MCTS sims for speed

  training:
    num_games_per_epoch: 50    # Fewer games per epoch due to model size
    max_epochs: 25             # More epochs for convergence
    eval_games: 20             # Fewer eval games
    checkpoint_freq: 5

  meta_learning:
    enabled: true               # Enable meta-learning for parameter adaptation
    adaptation_freq: 5         # Adapt parameters every 5 epochs

  reward_shaping:
    enabled: true               # Enable advanced reward shaping
    adaptive: true              # Use adaptive reward weights

  attention:
    move_encoder: true          # Enable move encoding and attention
    legal_masking: true         # Enable legal move masking
    cross_attention: true       # Enable cross-attention between board and moves

# Magnus Transformer + GRPO (SMOKE TEST - Quick validation)
magnus_transformer_smoke_test:
  device: "mps"        # Use MPS for Apple Silicon GPU acceleration
  model:
    type: "magnus_transformer"  # Fixed model type mapping
    input_channels: 19
    d_model: 512       # Optimized embedding dimension
    nhead: 8           # 8 attention heads for balance
    num_layers: 12     # 12 transformer layers (deep but efficient)
    dim_feedforward: 2048  # Efficient feedforward dimension
    baseline_checkpoint: "checkpoints/baseline_magnus_transformer_fresh.pt"

  grpo:
    group_size: 4      # Smaller groups for quick test
    clip_epsilon: 0.2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    learning_rate: 1e-4  # Higher LR for faster learning in test
    max_grad_norm: 0.5
    ppo_epochs: 3      # Fewer epochs for speed
    batch_size: 16     # Smaller batch
    num_workers: 2     # 2 workers for 6 games (3 games each)
    mcts_simulations: 200  # Very few simulations for speed

  training:
    num_games_per_epoch: 10   # Just 10 games for smoke test
    max_epochs: 1             # Just 1 epoch
    eval_games: 5             # Just 5 evaluation games
    checkpoint_freq: 1        # Save checkpoint every epoch

  meta_learning:
    enabled: false             # Disable for smoke test speed

  reward_shaping:
    enabled: false             # Disable for smoke test speed

  attention:
    move_encoder: true         # Enable move encoding and attention
    legal_masking: true        # Enable legal move masking

# Medium Transformer + GRPO (25M Parameter Model - BALANCED TEST)
medium_transformer_grpo:
  device: "mps"        # Use MPS for Apple Silicon GPU acceleration
  model:
    type: "medium_transformer"  # Medium transformer for balanced performance
    input_channels: 19
    d_model: 384       # Smaller embedding dimension
    nhead: 6           # 6 attention heads
    num_layers: 6      # 6 transformer layers
    dim_feedforward: 1536  # Smaller feedforward dimension
    baseline_checkpoint: "checkpoints/baseline_medium_transformer_fresh.pt"

  grpo:
    group_size: 6      # Medium groups
    clip_epsilon: 0.2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    learning_rate: 5e-5  # Slightly higher LR for smaller model
    max_grad_norm: 0.5
    ppo_epochs: 4      # Fewer epochs
    batch_size: 48     # Larger batch for smaller model
    num_workers: 3     # More workers for smaller model
    mcts_simulations: 150  # Fewer MCTS sims

  training:
    num_games_per_epoch: 40    # More games per epoch
    max_epochs: 20             # Fewer epochs
    eval_games: 15             # More eval games
    checkpoint_freq: 3

  meta_learning:
    enabled: true               # Enable meta-learning
    adaptation_freq: 3          # Adapt parameters every 3 epochs

  reward_shaping:
    enabled: true               # Enable advanced reward shaping
    adaptive: true              # Use adaptive reward weights

  attention:
    move_encoder: true          # Enable move encoding and attention
    legal_masking: true         # Enable legal move masking
    cross_attention: false      # Disable cross-attention for speed






